{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author Dennis\n",
    "\n",
    "Notebook generato a partire dall'esempio presente in https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/.\n",
    "\n",
    "L'esempio mostra le funzioni base di spaCy e come estendere i modelli di NER quando ci servono cose custom come l'introduzione di una nuova label, cosa che ci è utile per i nostri task. \n",
    "\n",
    "Durante lo studio è emerso il problema del forgetting https://github.com/explosion/spaCy/discussions/9414 quando si cerca di fare incremental learning.\n",
    "Questo problema, apparentemente, sembra non avere soluzione. È consigliato eseguire un retraining da 0 con spaCy. \n",
    "Altri blog dove si discute del problema: https://support.prodi.gy/t/generating-examples-in-spacy-to-address-catastrophic-forgetting/5097.\n",
    "E anche: https://github.com/explosion/spaCy/discussions/5134\n",
    "\n",
    "Una possibile soluzione (devo ancora leggerla): https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "\n",
    "A livello teorico si parla di catastrophic interference https://en.wikipedia.org/wiki/Catastrophic_interference\n",
    "\n",
    "Corpus originali sui quali si è fatto il training: https://spacy.io/models/it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load di un modello spacy\n",
    "import spacy\n",
    "\n",
    "# usually, the model is saved in the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the model presents the ner step in its pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case the model does not have ner among its steps\n",
    "# nlp.add_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebbene spaCy abbia nella sua pipeline built-in il ner per la entity recognition, e sebbene sperabilmente performi bene, non è sempre accurato per il nostro testo (in particolare, i modelli in italiano sono più limitati rispetto i modelli in inglese). In particolare, con l'italiano ma anche altre lingue, succede che la categoria che vogliamo possa non essere built-in in spaCy. \n",
    "\n",
    "Di seguito un esempio di come la NER (in inglese) performa su di un articolo riguardo una compagnia di E-commerce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a variable with the whole text\n",
    "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
    "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
    "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
    "Amazon - One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
    "\n",
    "Flipkart - Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
    "\n",
    "Snapdeal - Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
    "\n",
    "ShopClues - Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
    "\n",
    "Paytm Mall - To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace - Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app - Paytm. \n",
    "\n",
    "Reliance Retail - Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
    "Big Basket - India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services - express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
    "\n",
    "Grofers - One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
    "\n",
    "Digital Mall of Asia - Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
    "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# give the text to the nlp object, obtain the Doc object in output\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# now print the named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo come dal tutorial sembrasse che l'entità Flipkart fosse mal classificata in una PER (persona) invece che in una ORG. Invece, da qui vediamo che è stata correttamente classificata in una ORG. Evidentemente, nel tempo spaCy ha aggiornato i suoi modelli e sono migliorati. \n",
    "\n",
    "Nell'esempio successivo vediamo comunque come la rete, ancora oggi, non è in grado di identificare l'entità \"Alto\", né tantomeno quindi di classificarla come un prodotto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n"
     ]
    }
   ],
   "source": [
    "# a test to see that, as it is, the model cannot classify 'Alto' ad a product (a car) \n",
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quello che si fa qui è prendere un modello di spaCy pre-trained e farvi l'update con nuovi esempi. Quindi, il primo step è fare il load di un modello contenente la componente `ner`, da questo modello si estrapola il Named Entity Recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# get the NER pipeline component\n",
    "ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo ora come si può fare per eseguire l'update della parte NER in base al contesto e requirements dove lavoriamo. \n",
    "A questo punto, serve fare l'update del modello con nuovi esampi. Questi devono essere molti e significativi affinché il sistema migliore. Si parla di alcune centinaia come valore minimo. \n",
    "\n",
    "spaCy accetta training data come una lista di tuple. Ogni tupla dovrebbe contenere del testo e un dizionario. Il dizionario contiene gli indici di start ed end della named entity all'interno del testo (si usa notazione ad indici. Il primo e' l'indice della prima lettera/carattere della entity, il secondo e' l'indice del primo carattere al di fuori dell'entity), e la categoria/label della entità stessa. Un esempio è il seguente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a one-tuple example of the data format used by spaCy\n",
    "tuple_example = (\"Walmart is a leading e-commerce company\", {\"entities\" : [(0, 7, \"ORG\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(10, 17, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(31,37, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\"), (25,31,\"ORG\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(15,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(15,20, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(17,22, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(11,17, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(11,15, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(11,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(14,22, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(15,20, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(15,20, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,30, \"ORG\")]})\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È possibile che ci siano delle label/classi tra quelle usate nei dati di training non attualmente presenti tra quelle riconosciute dal modello. Pertanto, prima di qualsiasi operazione di update, aggiorniamo la lista di label a disposizione del nostro modello. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the new labels to the ner component of the pipeline\n",
    "for _, annotations in TRAIN_DATA:\n",
    "  for ent in annotations.get(\"entities\"):\n",
    "    ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is possible to list the labels currently considered by the model\n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo i dati di training (qui pochi ma è giusto per iniziare da qualche parte) per fare l'update del modello. In particolare, ricordiamoci, prima di fare il training, che a parte per la componente `ner`, il modello contiene altre componenti nella pipeline. Queste componenti non devono essere influenzate durante il processo di training. Dobbiamo pertanto disabilitarle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "# pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "pipe_exceptions = [\"tok2vec\", \"ner\"] # components that will be affected (only ner would have been ok too)\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions] # components that won't be affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unaffected_pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune osservazioni dal tutorial onlne:\n",
    "\n",
    "a)  To train an `ner` model, it has to be looped over the example for a sufficient number of iterations. If you train it for like just 5 or 6 iterations, the operation may not be effective.\n",
    "\n",
    "b) Before every iteration it’s a good practice to shuffle the examples randomly through `random.shuffle()` function. This will ensure that the model does not make generalizations based on the order of the examples.\n",
    "\n",
    "c) The training data is usually passed in batches.\n",
    "\n",
    "***\n",
    "\n",
    "È possibile chiamare la funzione `minibatch()` di spaCy sul set dei training data come fatto qui sotto. Essa ritorner' il dataset diviso in batches. Essa prende un primo parametro `size` per denotare la dimensione del singolo batches. \n",
    "A nostra volta, per ottenere il parametro size della funzione minibatch, usiamo un'altra funzione, `compounding`. Questa funzione prende tre input, `start` (il più piccolo valore che può essere generato), `stop` (il più grande valore che può essere generato), e `compound`, il compounding factor per la serie, Maggior informazioni riguardo questi aspetti  __[qui](https://spacy.io/api/top-level#util)__.\n",
    "\n",
    "***\n",
    "\n",
    "Ad ogni iterazione, il modello di ner viene aggiornato tramite la funzione `nlp.update()`, la quale ha come parami un array di esempi, composti da documento ed annotazione. Il `drop`, ossia il dropout rate, ossia la percentuale di neuroni randomicamente distrutti come metodo di regolarizzazione della rete. Infine, il parametro `losses`, ossia un dizionario che contiene le losses osservate su ogni componente della pipeline, usato per la backtrack propagation. Per questo, serve passare come parametro un dizionario vuoto. Ci pensa la funzione a riempirlo.\n",
    "\n",
    "***\n",
    "\n",
    "Ad ogni iterazione il metodo update fa una predizione. consulta le annotazioni per vedere se ci ha azzeccato e, se non lo ha fatto, aggiusta i pesi nella NN affinché l'azione corretta abbia uno score più alto la prossima volta. \n",
    "Infine, il training è fatto solo sulla componente nlp, o comunque sulle componenti che abbiamo indicato come \"attive\". Le altre non vengono influenzate dal training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tok2vec': 0.0, 'ner': 1.2652465105542852}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.2651807615982587}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.2671112190950553}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.843149523724394}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.827805456960367}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.828644086123553}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.08635737124986}\n",
      "Losses {'tok2vec': 0.0, 'ner': 9.499662768721223}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.577408242699434}\n",
      "Losses {'tok2vec': 0.0, 'ner': 13.576520268507878}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.674196717269455}\n",
      "Losses {'tok2vec': 0.0, 'ner': 16.28660020716795}\n",
      "Losses {'tok2vec': 0.0, 'ner': 18.065558597145966}\n",
      "Losses {'tok2vec': 0.0, 'ner': 19.082501530245167}\n",
      "Losses {'tok2vec': 0.0, 'ner': 20.87836748761061}\n",
      "Losses {'tok2vec': 0.0, 'ner': 22.078238831484562}\n",
      "Losses {'tok2vec': 0.0, 'ner': 24.064810429303577}\n",
      "Losses {'tok2vec': 0.0, 'ner': 24.222429819703247}\n",
      "Losses {'tok2vec': 0.0, 'ner': 26.3127833759316}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0862783001430216}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8686390675857965}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.145851415147948}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.410738755998265}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.730200962330114}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.737286083763335}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.194802878421356}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.216669029659116}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.989751145952155}\n",
      "Losses {'tok2vec': 0.0, 'ner': 14.0495995155011}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.728251155238482}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.729764196545393}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.730639817592039}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.827695807731818}\n",
      "Losses {'tok2vec': 0.0, 'ner': 19.706581285139492}\n",
      "Losses {'tok2vec': 0.0, 'ner': 21.648786070483567}\n",
      "Losses {'tok2vec': 0.0, 'ner': 21.818899518454153}\n",
      "Losses {'tok2vec': 0.0, 'ner': 24.300892119172183}\n",
      "Losses {'tok2vec': 0.0, 'ner': 24.54776737885502}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2657907155421615}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.2590368315669114}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.269373492185357}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.941586223726054}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.969386575798399}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.971800728507606}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.82507024709567}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.836291345834604}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.442873205393747}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.57507597936219}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.654306218688003}\n",
      "Losses {'tok2vec': 0.0, 'ner': 13.22205781459655}\n",
      "Losses {'tok2vec': 0.0, 'ner': 13.4077427796631}\n",
      "Losses {'tok2vec': 0.0, 'ner': 16.757151296498055}\n",
      "Losses {'tok2vec': 0.0, 'ner': 18.41755607167457}\n",
      "Losses {'tok2vec': 0.0, 'ner': 18.903051324860716}\n",
      "Losses {'tok2vec': 0.0, 'ner': 19.937960352296187}\n",
      "Losses {'tok2vec': 0.0, 'ner': 19.958925322039732}\n",
      "Losses {'tok2vec': 0.0, 'ner': 20.85631239060222}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0068173592632939095}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.7185602552828771}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3952410012414012}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4029063572671303}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.0198808834470965}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.306984890457473}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.387760117385209}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.76124343252178}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.803552262861748}\n",
      "Losses {'tok2vec': 0.0, 'ner': 9.591785797165706}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.504020017960789}\n",
      "Losses {'tok2vec': 0.0, 'ner': 14.728351769352876}\n",
      "Losses {'tok2vec': 0.0, 'ner': 14.766993851426129}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.058333628030137}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.992540086787708}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.993621952430825}\n",
      "Losses {'tok2vec': 0.0, 'ner': 16.21957388884713}\n",
      "Losses {'tok2vec': 0.0, 'ner': 17.782107226451007}\n",
      "Losses {'tok2vec': 0.0, 'ner': 19.68763002319875}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.058030650840357745}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.7837990717211243}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.7892525830640356}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.8832530487855257}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.774146640830124}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.994786297968629}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.10096170644443}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.03884501605879}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.038908292755547}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.134455093155177}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.13445802972211}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.394210596286397}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.39956529427835}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.561634819147262}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.6191467026051}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.61987873093159}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.642375832890105}\n",
      "Losses {'tok2vec': 0.0, 'ner': 15.076580185823898}\n",
      "Losses {'tok2vec': 0.0, 'ner': 16.779174365611993}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.05025767854365171}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.2662532611565491}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.2752953568407004}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.8202427235264622}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.8488332925257627}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.8489006394921454}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.384606688833448}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.092946569014963}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.093201810518116}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.342511152298031}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.957764748423348}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.375129288874726}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.440497164804095}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.646957477396714}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.744624913369097}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.7649529382690705}\n",
      "Losses {'tok2vec': 0.0, 'ner': 9.475842728384215}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.394664318199027}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.5140014845281}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0012046354606963725}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.027035785423853646}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.952753933796447}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0578188691086052}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.057819175254326}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.049908091284264}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.340813276014957}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.5627709884497385}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.562829859809674}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.494650203088003}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.491391406280691}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.369034345548917}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.855079082231528}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.909580460982523}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.91121281985076}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.913621084187373}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.914203322221807}\n",
      "Losses {'tok2vec': 0.0, 'ner': 12.659730093045578}\n",
      "Losses {'tok2vec': 0.0, 'ner': 16.489647528260683}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7979016085949182}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.798051001299838}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.266010270659189}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.266012262671235}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.266058072893393}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3246211244914083}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.4390187973860167}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.295014550491637}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.299282013430395}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.514854025426906}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.514872097018362}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.969386295398162}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.973094092764074}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.974168123340394}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.983041078680958}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.106466956656726}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.9049188675426105}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.90491886830527}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.112418628108554}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.9176963729664214}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.9515205873868946}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.9515219887423253}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.9515229296244317}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.9516511276580689}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.9924043893852189}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.945136977632023}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.945153062481817}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.945154214738302}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.945362998208994}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.954630919032422}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.240136466977443}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.240716472159443}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.227935338338569}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.700284540967237}\n",
      "Losses {'tok2vec': 0.0, 'ner': 10.374236693378213}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.120349297888268}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.613636440090977}\n",
      "Losses {'tok2vec': 0.0, 'ner': 11.613657971356753}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.0674188726559816e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.415945810830007}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4780016341351672}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9644429668067003}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0521272261313195}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0521272333251135}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0532679446837108}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6747787860294165}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6948894681604685}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.074834839335679}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.074856610394322}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.074884194407592}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.074900202730131}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.950080078516103}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.026663022520369}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.03429666353042}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.0344262486215845}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.034481035147368}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.24263566871114}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.8751408065434065e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2632617738158909}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.692949523605304}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.563249248085315}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.563249347386791}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.563261531633134}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.5632615324204355}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.563305239656874}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.563305289305299}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.89048930435742}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.890489304666403}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.893107984811611}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.893109364812405}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.893110854650066}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.907136297796682}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.907336291422828}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.910561547716203}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.910599177909163}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.996842903715471}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0012885035274790672}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0012912788648774647}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0012980178451412984}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.021842829199450457}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.02832726157901093}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.028398298776919}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0285194389991887}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.028754541411908}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019677709183228}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019678123218678}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019715978189074}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019777523616081}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019777542251246}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019777553341275}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019777826488062}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019976284770416}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.019976284962014}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.992767500266221}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.99303763915794}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.564999201058087e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0010494161846403497}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0021028099797590514}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002121810889013298}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.12167568962858581}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3776808789688628}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.377812383922773}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37781243955658256}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37784329502708336}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3785248783522913}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37864221593645864}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37864223291137744}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3786487238156744}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37884671413640986}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.378846718370467}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37884744839872575}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37887062062676274}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.37887071155304153}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3788726362064231}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.010746171899883939}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.010749357731273156}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.010750117241825823}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.003744473996782}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.003906493131017}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.003906542797985}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8329786010618845}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.527535212976332}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.527942860147709}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.527942861437244}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.561083662831869}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.561098415039878}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.561098891686079}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.5618689968903325}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.5747114485415885}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.574714681607419}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.5752203529119395}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.575220356854191}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.57522151002705}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.953693209232529e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3465158401475707e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00032309654686907106}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0003232545703627093}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.04382408504895658}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.04382408615862186}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.1846259188165353}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.18462592298244251}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.1846262574319278}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.19242399787182218}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.1924240177157836}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.1928488309469522}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.1626374067682264}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.162637427479418}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.741935933182098}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.744686037608437}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.7446892158584038}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.7446928397207464}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.296446128740458}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.726812074503966e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.48794782059901115}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.48798092887640493}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4879809305295912}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.49207425349168205}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4920742536463683}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.49209217711997444}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1045069103733927}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1045566785754255}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.191096587821328}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1934731036812545}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1977101463225857}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1977107405837422}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1977107407041931}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0017389565293424}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.001738956912102}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0017389570780257}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.001738957288647}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.001738957352055}\n",
      "Losses {'tok2vec': 0.0, 'ner': 9.242653361529702e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0263688573440316e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.937975071224154e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0742113207492947e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0782591218139232e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.085674091173113e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.035604757861504015}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3613868482907625}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.36138684838208945}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.36138697724061153}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.36138697979150136}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3613869798134992}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.36138697982976925}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3613869800700039}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3613880878996366}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3613880880877525}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3613881003302369}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.36138946432412333}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3613621654231105}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.644242617373661e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.521210277980189e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.531390641818415e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5711645500516445}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5806689639311184}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5806856372565516}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5806919509484465}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1714197951875036}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1714197963547752}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1714197963928394}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1714198329027137}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1714198412107386}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.171419858564982}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.186645880975279}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.18664588218214}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1866501872521904}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.172196300221551}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.172196300284857}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.172198621630774}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.1669378383407434e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.681516854987893e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.05945970841437922}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.059459744781982946}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9670372594356553}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9670372600114372}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9670372600115604}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.81465393253063}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.995944782574002}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.9959447827484555}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.9959972129250296}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.99599723971413}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.0069212292811125}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.006923932453588}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.006924112607345}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.006979682813692}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.006979682815036}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.0069796828390984}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.00715408698556}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0005227249099528931}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0005227249173683468}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0005227249180037928}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0005227263992343218}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0005227353916035091}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0006432723482074931}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0006434037520211413}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0006434047365119503}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0008554721551379122}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9690481171896301}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9690481171901488}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0240141312752074}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6045420343176424}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.604589196939119}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6045893311019417}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6045893400245568}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6045893742392723}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6045893742654824}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.604589377464449}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.638494538646064e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00016189500673225357}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00016189516398454625}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00016189516398536482}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00016189654634502675}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0001618966989503188}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00016566169409911965}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00016585364794567827}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00016626570422241326}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0012969475384065707}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0012969476131364328}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0026017035177193573}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002648295860186467}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002648295904681502}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002648506348211829}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0026689237169124982}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0026689869890908105}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0026743729784001716}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002676396016939075}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.512579497110666e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.530527548981374e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5099406435628482}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5100178277288867}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5100178277409553}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5100178277409797}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.510017827741176}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5101465521060893}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.9991782330318624}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.9991782330319405}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.999178706309869}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.999180419999597}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.9991804224963827}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.962863366145923}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.962863366359219}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.962863383774405}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.962939955071994}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.962939955072129}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.962939955120098}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.32002992104941885}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.32003004033952503}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.32020164673855217}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.32020493462036653}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.40914127678937684}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4091412771718955}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.385011171223689}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3850147168561566}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.667940024613376}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6679400248469385}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6679400268442963}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6679400268443176}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6679400324338434}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6679400324750664}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6679400324764053}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6701070251460157}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.670107025484772}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6701072025667734}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.670107894304659}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1477781910656078e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2901820739023146e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2947263447975532e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2947295476454152e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.000972365749516772}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009723658459540642}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009725610260390435}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009725629721539545}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009725996494326031}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009725996498756145}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.2367140973223285}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7764045922057812}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7764045922058602}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.038036826073551}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0380368262040136}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.038065198167752}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0380651981678244}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.494299414858203}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.4942998705946335}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.736324192706342e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.051790219281623e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002438973431451291}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024389734314514923}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024389734314853016}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002441724949117066}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024417249521337973}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024417250094542773}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002441748757985677}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.002441748758007738}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024417557470789316}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024417886613010142}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024417886613010164}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0024417886614815612}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.003251985997763936}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.003251985998553368}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00325198600067319}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.000827507470194}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0008275101286443}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.1290450653324265e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.389064893958865e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.46516915696748e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.8825939448791177e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.034400273617465185}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.03440027381384706}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.034400273813848026}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.034400273814392666}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.03440027381483688}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.1501246436984319}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.150124643717892}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.15012464371872214}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.15012464492795968}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3389780396589641}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3395592720895853}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3504750729385604}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3504750730958957}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3504750736930953}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.350475073695722}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0852241360995845e-17}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2091034547759798e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828401974325067}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.982840207746661}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828402077814695}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828402090726385}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828402465948294}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828583445735215}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828585688614404}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828585688622633}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.982862880704393}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828628974041036}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828642925754016}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.982864469408712}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828650048090082}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828650048157785}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828650073975957}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9828650073984515}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9829053188658081}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0160170240765269}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.016017039003266918}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.01601721530737929}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.10621119635762503}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.10624476948871339}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.10624476948876671}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.47267960739700343}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.47267960759766336}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4726796388717161}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4726796412406373}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4998692551621118}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5026364918019987}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5026364936735669}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5026364936735669}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.502695982810038}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5026959828106397}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5026959934013621}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5026959934026264}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.6902804233819508}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.982904740476766e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.467100739345841e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.197463443199978e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2799696910213598e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2859444704501119e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3245373011633214e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.6209069086498815e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.901250659914967e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.093587066075747e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007222548130235125}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007222548138997392}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007222872052056118}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007222904493546166}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007222988225795598}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007866797920042068}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007866863282175468}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007866863357547257}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007866863370995171}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007866863505993908}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594894931633282}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594894931828872}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594895211122332}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594895211122474}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594895211123287}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.959499401355273}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594994014033051}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594994014058194}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594994015200091}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594994015200118}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594994736043228}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.959499473604428}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.959499473654891}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9594994736551565}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.95949947365879}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3389365792792267}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.4853781507591175}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.485378238158352}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.4853782381622915}\n"
     ]
    }
   ],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# TRAIN THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    for iteration in range(30):\n",
    "        # shufling examples  before every iteration\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                # Update the model\n",
    "                nlp.update([example], losses=losses, drop=0.5)\n",
    "                print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che il training è concluso, si può testare su dei nuovi dati. Se il modello continua a non operare come ci aspettiamo, potrebbe essere necessario aggiungere altri dati al training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Alto', 'PRODUCT')]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model - now the model correctly classifies Alto!!\n",
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come vediamo dalla cella qui sopra, il modello, adesso che è stato aggiornato, è in grado di riconoscere Lto e di classificarlo correttamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to english_model\n",
      "Loading from english_model\n",
      "Entities [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Save the  model to directory\n",
    "output_dir = Path('./english_model')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training di una nuova entity type (label) in spaCy\n",
    "\n",
    "Supponiamo di avere una categoria che non è ancora presente, per il momento. Immaginiamoci quindi una nuova categoria FOOD. spaCy permette di aggiungere una nuova categoria e di fare il training del modello. Questa feature è utile in quanto ci permette di aggiungere nuove entità, migliorando task come information retrieval. Si parte facendo il load di un modello spaCy pre-esistente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load the spacy model\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Getting the ner component\n",
    "ner=nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serve preparare ora la nuova label e i nuovi training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,15, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo passo è aggiungere la label FOOD al modello, attraverso il metodo add_label(). Come prima, inibiamo le parti della pipeline che qui non ci interessano. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label to the ner component\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che: \n",
    "a) Serve eseguire un numero sufficiente di iterazioni al modello perché le modifiche siano significative. \n",
    "\n",
    "b) Serve inoltre fare il fine-tuning del modello con un occhio alle performance. Prima di ogni iterazione, è meglio eseguire lo shuffle degli esempli, in maniera da non introdurre bias proveniente dall'organizzazione degli examples.\n",
    "\n",
    "c) i training data vanno passati in batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tok2vec': 0.0, 'ner': 3.9288996426526424e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.852108600148923e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8541110670177405e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6537065975908272}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6537065977106993}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6537097684635291}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6537104686822603}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.658817139959233}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6588172068667404}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.658818181673252}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.658818183332906}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6588181833742635}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6588182390713566}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6590123154325729}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3396537902166091}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3417269009885401}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.34172691737243993}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3417271712484607}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.3417271715369572}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2150820833545615}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.215082310362314}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2150823122163124}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.21508241073127}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2150828409190304}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2150828413022363}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2150828424666138}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.215082842467557}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2150828424905613}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4057550916559352e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.604419485060581e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.156588022192367e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.512315775744365}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5123214525794474}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.512331841105183}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5170433969859306}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5170434967591151}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5170434967765288}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5170508282469777}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5170542279321461}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5170561372696288}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5170561372706518}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.9773185886401591}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.181951016781507}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1608653416521735}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.16086534165791}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.160865341657991}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1608653423600432}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.160865342427695}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1608653532221886}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.160865353473375}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.1608655498644183}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.280785439268648}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.3050366643679547}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.3051110625777413}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.495705495150895}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.495705553406908}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.6760370891081617e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.1474017682656625e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.5011334618641124e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.01364563702639281}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3936966245419917}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.393696624544115}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3956643766008585}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.395664376643073}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.37506977980316}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.375069793711494}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.375069793912776}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.37506980246675}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.375069808100884}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.3750698226688196}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7090746009279467e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7186133610266678e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.719273296227472e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7192988054049931e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7207179121481655e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.361155920439121}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.361155941159429}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.361156423352831}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.361156439547754}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3611564417033506}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.361156441777782}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3611564935913725}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.361156878599276}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.3611573137763164}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.5742226189166855e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.015241052841224364}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.015241067945041823}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.015241071660866143}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.108606442889306}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.108664749622999}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.108664759765765}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.1086647597661274}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.1086648118590743}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.1086648118632008}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.1086648167693447}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.108684924993958}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.1086849439629605}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.108684943991426}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.561789460464152e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.582413166025627e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.084212799233031e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.09541985454821e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3866734548987334}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3866734551995776}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3866734609189708}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.386673461386029}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.386673461932941}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3866734629031245}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.386673463991018}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.38667346399182}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3866734646230878}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3866734699443928}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3984529611581013e-12}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.8245367323301102e-12}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906658243938351}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906658247407817}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.49066582541309256}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906658254132751}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906658261458075}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906706179041407}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.49067061865523603}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906706187001041}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906707077238037}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.49067071114865635}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4906712521158524}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.49067142097530075}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8194381387482837e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8194474227471275e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.202393877313323e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.202427834850169e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.204429644528466e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.204440551056737e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.204440917070283e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.2058907306228334e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0018391359293440995}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.001839158281554954}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0018391981941975684}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4434644214854746}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.4434646156171938}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.44346461723941627}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.297016807370409e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.297093216261407e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.297850530837985e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.297871942144804e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.29788168510204e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.298003771494508e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.2980038732134e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.298055031043987e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.48539659442826e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.487380631852737e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.487387893253416e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.488150757830337e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7060573417584328}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.7060573417667753}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.067911461407573e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.05477161859716e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.177836548772949e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.17939267155181e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.003610883034480298}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004543817220734702}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004543817244355755}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004543817727017296}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00454729003801818}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004548484452702201}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004548509533682256}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004548515644230358}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004548526805169714}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.004548583919210555}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.9438770751413717e-13}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.5593443319641374e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.5600442072434844e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0001583710542102138}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0001583711953614442}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0001803357651660202}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.008156188114939596}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.008156188207300278}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.008156193683734157}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.008156253612059947}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.008156253620406395}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.008158537430997282}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0081586182671658}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.008158618270614915}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.762048122204416e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9566827846720177e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9799607384835813e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0467508033753134e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0470427588375942e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0519652633588927e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.01593299712399562}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.015933115950736893}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.015933138252930128}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.015936920679706156}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.01593692404353007}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.01593692415337884}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.01593692417077664}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.015936924170888715}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.103452257820249e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.155972129304181e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.190963463878151e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0008824350415171661}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0008832831424126124}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0008832837955970496}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0008832890212431048}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0008917168187691727}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0008917176207628496}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009095250034466718}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009095262978731661}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009095640279660623}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009095646686704609}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0009132137983391108}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3071113464117574e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.539021559818846e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0003979531372571793}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0004055671767902844}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.000405588357559841}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.006211902185201393}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.006211902869847998}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.006218578938402166}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0062185929957304115}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.006218593619850021}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.006218698427026603}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.006219275454586289}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00622093486423923}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.006221029606791731}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.485187425727989e-15}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.540633771111112e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.540803504713291e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.5409393841530283e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.562852185727554e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.0599728386981204e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.0608604080778327e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.0609251093299333e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.060925971728575e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.0609259947161774e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.062539476294649e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.062555154576492e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.0625551907166836e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.0625556187315995e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.623166913911652e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8607582506035005e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1558075058695677e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1580823966513537e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 9.211548361780543e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.078865039287847e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0788671027328587e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.0914422893496658e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2928098284486092e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6379928717122606e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6379979710869655e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6386407822738498e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6651033018074428e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.6651034014304675e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.695924898710862e-13}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.303057037158754e-13}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.7158189293060725e-12}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.001022282620919843}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.001022366367017728}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0010238520019009083}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0010238524343906233}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.001075797019815604}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0010757970219778157}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0010757970283512895}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5986480830441131}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.598648083050658}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5986480834008937}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.5986480834009678}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.688973527591641e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.919948400974702e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.922143715137973e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.923978265412263e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.987872243443867e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 9.154575442725863e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2882484142067452e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3639944030905784e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.472188844286324e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4738930854755355e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4975446865263198e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.567390741518373e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9426825919233753e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0694533263941496e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.127093878368247e-12}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.205656552234209e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.2064048055680262e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.206404821432541e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.2066356795760792e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.820927945267012e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.096248628767301e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.096396656330256e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.096399491909462e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.293193811587269e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.293194013095949e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.294927190312536e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.294932365902614e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.29493240682872e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.315263881307883e-12}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9479633203812368e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.952499158269246e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.060713813566097e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.183820921707255e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.234825164677635e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.280041497125248e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.280042384300014e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007572088593456377}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.000757210218203868}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007572103828948832}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007572104397687307}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007597136564802324}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0007597168178239606}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4401930816034754e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.056348863607815e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.945232703823167e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.985459843971232e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.985463606660214e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.5007630168879106e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.358224531781473e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.358224532078761e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.35824069703364e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.47626646665076e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 9.502283888102262e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00010314887808087594}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0001032285917692903}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.00010322990730009364}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.518915585091989e-13}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.747795080914229e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2697157470151338e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2697730883350083e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5420269108501918e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.554046425185817e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.562445844943882e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.563747085325528e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.563899333142337e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0074206282253339115}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.007420628225679575}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.007420857020950962}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.0074212828726104}\n",
      "Losses {'tok2vec': 0.0, 'ner': 0.007421283879503143}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.604648178077178e-09}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.237430678403695e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.237430694596304e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2374951360428073e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2598879242596158e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.259891558871196e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2714435497841827e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.271455072388031e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.2714876163764042e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.743987616873392e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.74410347613191e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.7443973366111556e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 5.579841197863913e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.008259631018672e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.839341147798657e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8413342873259597e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8413354544984284e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8413355716132203e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8416405068001748e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.841640913957476e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8416409140275008e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.8416616123695564e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.9613365606263853e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9292714963768256}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.9292714963768545}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.929271496717707}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.929271515033694}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.929271527484551}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4015973706802584e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4015974912236392e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4016208733075596e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4016219660105288e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.401621987062787e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.4018486567992769e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.402001609074684e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.626652504989096e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0254732151984862e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.0257055799531274e-07}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1194122216824913e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1194710066176652e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1263336073673738e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.1269175021655675e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.125729256532527e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.346616827058955e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.347944746084038e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.34794500802519e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.3626724542324966e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.7394048780968825e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3499310434497003e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3499310490453309e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3499436208613864e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3499436670230778e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3499436680191609e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3499438392287272e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.3499574807049832e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.484517631622914e-05}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.221815502142547e-12}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.9678128797449615e-11}\n",
      "Losses {'tok2vec': 0.0, 'ner': 4.5538319953889674e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.329421138294182e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.329421327785851e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.333192838203172e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 6.333401282173982e-08}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5172076879070225e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5176127659856799e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5176168350766487e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5176171377704805e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5180698687606672e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.518069985419608e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 1.5224073430250939e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.533603241981184e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.533762707644905e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.577547370586016e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 8.481664744387241e-10}\n",
      "Losses {'tok2vec': 0.0, 'ner': 3.2100572523067144e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.318444962100353e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.31855602127134e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.318694179221801e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.320994740944921e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.320994748912361e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.320994749022508e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.320995527940625e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 7.320997025115405e-06}\n",
      "Losses {'tok2vec': 0.0, 'ner': 2.9481886819388653e-05}\n"
     ]
    }
   ],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "  # Training for 30 iterations     \n",
    "  for itn in range(30):\n",
    "    # shuffle examples before training\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(1.0, 4.0, 1.001))\n",
    "    # ictionary to store losses\n",
    "    losses = {}\n",
    "    for batch in batches:\n",
    "      for text, annotations in batch:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        # Update the model\n",
    "        nlp.update([example], losses=losses, drop=0.5)\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "Sushi FOOD\n",
      "Maggi FOOD\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "\n",
    "# make it a doc with the new nlp model\n",
    "doc = nlp(test_text)\n",
    "\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in the original article\n",
      "India FOOD\n",
      "billion FOOD\n",
      "billion FOOD\n",
      "12% PERCENT\n",
      "2021 DATE\n",
      "Amazon FOOD\n",
      "Prime FOOD\n",
      "Limited FOOD\n",
      "Founded FOOD\n",
      "Flipkart FOOD\n",
      "Amazon FOOD\n",
      "Walmart FOOD\n",
      "Started FOOD\n",
      "Snapdeal FOOD\n",
      "Unicommerce ORG\n",
      "ShopClues FOOD\n",
      "2011 FOOD\n",
      "Gurugram FOOD\n",
      "Partners FOOD\n",
      "Global FOOD\n",
      "Ventures FOOD\n",
      "Paytm FOOD\n",
      "Mall FOOD\n",
      "Paytm FOOD\n",
      "Reliance FOOD\n",
      "Reliance FOOD\n",
      "Basket FOOD\n",
      "Moreover FOOD\n",
      "Grofers - FOOD\n",
      "Grofers FOOD\n",
      "Going FOOD\n",
      "Asia ORG\n",
      "DMA ORG\n"
     ]
    }
   ],
   "source": [
    "# let us also test the original document to see that there is no forgetting\n",
    "doc = nlp(article_text)\n",
    "print(\"Entities in the original article\" )\n",
    "for ent in doc.ents:\n",
    "  print(ent, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('NER')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e92c8a08333f1ec85557c60f46a2c9c6f09b9867acc2a37086294b24343c3efc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
